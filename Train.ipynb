{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training AI",
   "id": "c773a03f658324de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries and setting config variables",
   "id": "9a7afd7b75072d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:15:59.543890Z",
     "start_time": "2024-06-13T17:15:50.150060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import uuid\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Masking\n",
    "from tensorflow.keras.regularizers import l2"
   ],
   "id": "72039c7e52f8ea72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 19:15:53.972961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 19:15:56.632616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sequence and Shape Settings",
   "id": "e9c945c09725e4c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:15:59.556896Z",
     "start_time": "2024-06-13T17:15:59.547184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_length = 30  # Number of frames per sequence\n",
    "shape = (sequence_length, 258)\n",
    "num_classes = 4"
   ],
   "id": "73de93821ac6241c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Path and actions",
   "id": "9f9aa6199c53c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T17:15:59.562885Z",
     "start_time": "2024-06-13T17:15:59.558988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actions = ['hallo','du','idle','a']  # List all your actions including idle\n",
    "DATA_PATH = 'CollectionBase/MP_Data/30frame_no_face_+idle'  # Update with path to training data"
   ],
   "id": "4a99d2050f24f8de",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e8704d17516bca93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Activated landmarks",
   "id": "fa8092ccf2429682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "landmarks = [False,True,True,True] # Select True/False for different landmarks \n",
    "# (face, pose, left hand, right hand)"
   ],
   "id": "1e5dd645801cffbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drawing Landmarks",
   "id": "79de2d00f18f22f8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T10:40:43.804771Z",
     "start_time": "2024-06-13T10:40:43.795331Z"
    }
   },
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    arr = []\n",
    "    if landmarks[0]:\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "        arr.append(face)\n",
    "    if landmarks[1]:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        arr.append(pose)\n",
    "    if landmarks[2]:\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        arr.append(lh)\n",
    "    if landmarks[3]:\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        arr.append(rh)\n",
    "    return np.concatenate(arr)\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    if landmarks[0]:\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    if landmarks[1]:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    if landmarks[2]:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if landmarks[3]:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if landmarks[0]:\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                  mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                  mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                  )\n",
    "    if landmarks[1]:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "    if landmarks[2]:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "    if landmarks[3]:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                  )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labeling Data",
   "id": "c7d411789a7c25e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T10:40:47.902107Z",
     "start_time": "2024-06-13T10:40:45.398641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "# Label -> Number mapping\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "# Load data\n",
    "for action in actions:\n",
    "    action_dir = os.path.join(DATA_PATH, action)\n",
    "    for sequence in np.array(os.listdir(action_dir)).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            frame_path = os.path.join(action_dir, str(sequence), f\"{frame_num}.npy\")\n",
    "            res = np.load(frame_path)\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "# /TODO: Maybe higher test size\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ],
   "id": "23c4cf3e39fd9d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 120\n",
      "X shape: (120, 30, 258)\n",
      "y shape: (120, 4)\n",
      "X_train shape: (114, 30, 258), y_train shape: (114, 4)\n",
      "X_test shape: (6, 30, 258), y_test shape: (6, 4)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Version 0.1.0\n",
   "id": "6e269a203136e315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# region Version 0.1.0\n",
    "# Define the L2 regularization factor\n",
    "l2_factor = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=shape))\n",
    "\n",
    "# Adding LSTM layers with L2 regularization\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Fully connected layer with L2 regularization\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer for num_classes with L2 regularization\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "# Set up CSVLogger\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "# Set up TensorBoard\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]\n",
    "# endregion"
   ],
   "id": "85512f50e9a296d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cda9bdac77566c7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compact LSTM (RNN)",
   "id": "17eebef00fb158ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T11:14:57.175727Z",
     "start_time": "2024-06-05T11:14:57.173786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# region Small LSTM\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax')) \n",
    "# endregion"
   ],
   "id": "99053e38ab77cfe3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Most accuracte",
   "id": "2f8da0e6594a70e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T06:13:35.455006Z",
     "start_time": "2024-06-10T06:13:23.216461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the L2 regularization factor\n",
    "l2_factor = 0.00\n",
    "dropout = 0.2\n",
    "# These two are for better generalization dropout is 20% and L2 regularization is 0.00\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Masking 0.0 values because keypoints that are not on the screen are 0.0\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "\n",
    "# Statistics\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]"
   ],
   "id": "9dd7a17a1feeefa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 340ms/step - accuracy: 0.4755 - loss: 1.1995 - val_accuracy: 0.3333 - val_loss: 1.3325\n",
      "Epoch 2/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 117ms/step - accuracy: 0.8346 - loss: 0.4893 - val_accuracy: 0.6667 - val_loss: 1.2758\n",
      "Epoch 3/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 109ms/step - accuracy: 0.8742 - loss: 0.3268 - val_accuracy: 0.1667 - val_loss: 1.3075\n",
      "Epoch 4/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step - accuracy: 0.9539 - loss: 0.1995 - val_accuracy: 0.0000e+00 - val_loss: 1.3063\n",
      "Epoch 5/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - accuracy: 0.9735 - loss: 0.1045 - val_accuracy: 0.0000e+00 - val_loss: 1.2992\n",
      "Epoch 6/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - accuracy: 0.9787 - loss: 0.0708 - val_accuracy: 0.0000e+00 - val_loss: 1.3401\n",
      "Epoch 7/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step - accuracy: 0.9429 - loss: 0.0883 - val_accuracy: 0.6667 - val_loss: 1.2095\n",
      "Epoch 8/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step - accuracy: 0.9394 - loss: 0.1494 - val_accuracy: 0.3333 - val_loss: 1.2918\n",
      "Epoch 9/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step - accuracy: 0.9794 - loss: 0.0433 - val_accuracy: 0.1667 - val_loss: 1.4328\n",
      "Epoch 10/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 105ms/step - accuracy: 0.9909 - loss: 0.0307 - val_accuracy: 0.0000e+00 - val_loss: 1.5835\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step - accuracy: 0.6667 - loss: 1.2095\n",
      "Test loss: 1.209513783454895\n",
      "Test accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Execution",
   "id": "b7798f6fcef31617"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ],
   "id": "b79c1710389baf2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T18:58:22.865106Z",
     "start_time": "2024-06-09T18:58:22.802382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id = uuid.uuid4()\n",
    "tf.keras.models.save_model(model, f\"ai_{id}.keras\")"
   ],
   "id": "7d1fdf6122d61146",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing with OpenCV",
   "id": "2070e80b9fcf7e72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "model = tf.keras.models.load_model('ai_fixed.h5')\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Mirror the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    # Ensure that the size of 'res' and 'colors' is the same\n",
    "    if len(res) != len(colors):\n",
    "        print(\"Size of 'res' and 'colors' does not match. Please check your inputs.\")\n",
    "        return output_frame\n",
    "    for num, prob in enumerate(res):\n",
    "        max_prob = np.max(prob)  # get the maximum probability\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(max_prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "colors = [(245,117,16), (117,245,16), (80,22,10), (121,22,76)]\n",
    "cap = cv2.VideoCapture(0)  # Start the webcam\n",
    "ret, image = cap.read()  # Read a frame from the webcam\n",
    "cap.release()  # Close the webcam\n",
    "\n",
    "#plt.figure(figsize=(18,18))\n",
    "#plt.imshow(prob_viz(res, actions, image, colors))\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    collecting = False\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Check if 'g' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('g'):\n",
    "            collecting = True\n",
    "            sequence = []  # Reset the sequence\n",
    "            frame_count = 0  # Reset the frame count\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        if collecting and frame_count < 30:\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(actions[np.argmax(res)])\n",
    "                predictions.append(np.argmax(res))\n",
    "                collecting = False  # Stop collecting after 30 frames\n",
    "\n",
    "                #3. Viz logic\n",
    "                if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                    if res[np.argmax(res)] > threshold:\n",
    "\n",
    "                        if len(sentence) > 0:\n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\n",
    "                'q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "58f93b36d43473f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
