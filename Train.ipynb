{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training AI",
   "id": "c773a03f658324de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries and setting config variables",
   "id": "9a7afd7b75072d74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:30.412900Z",
     "start_time": "2024-06-14T10:49:24.287379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import uuid\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Masking\n",
    "from tensorflow.keras.regularizers import l2"
   ],
   "id": "72039c7e52f8ea72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 12:49:25.951928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 12:49:27.551182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sequence and Shape Settings",
   "id": "e9c945c09725e4c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:30.415942Z",
     "start_time": "2024-06-14T10:49:30.413918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_length = 30  # Number of frames per sequence\n",
    "shape = (sequence_length, 1662)\n",
    "num_classes = 2"
   ],
   "id": "73de93821ac6241c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Path and actions",
   "id": "9f9aa6199c53c50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:31.588417Z",
     "start_time": "2024-06-14T10:49:31.586132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actions = ['j','hallo']  # List all your actions including idle\n",
    "DATA_PATH = 'CollectionBase/MP_Data/full'# Update with path to training data"
   ],
   "id": "4a99d2050f24f8de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e8704d17516bca93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Activated landmarks",
   "id": "fa8092ccf2429682"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:33.430034Z",
     "start_time": "2024-06-14T10:49:33.427750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "landmarks = [True,True,True,True] # Select True/False for different landmarks \n",
    "# (face, pose, left hand, right hand)"
   ],
   "id": "1e5dd645801cffbb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drawing Landmarks",
   "id": "79de2d00f18f22f8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:34.678920Z",
     "start_time": "2024-06-14T10:49:34.667001Z"
    }
   },
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    arr = []\n",
    "    if landmarks[0]:\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "        arr.append(face)\n",
    "    if landmarks[1]:\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        arr.append(pose)\n",
    "    if landmarks[2]:\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        arr.append(lh)\n",
    "    if landmarks[3]:\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        arr.append(rh)\n",
    "    return np.concatenate(arr)\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    if landmarks[0]:\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    if landmarks[1]:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    if landmarks[2]:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if landmarks[3]:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if landmarks[0]:\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                  mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                  mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                  )\n",
    "    if landmarks[1]:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "    if landmarks[2]:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                  )\n",
    "    if landmarks[3]:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                  )"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labeling Data",
   "id": "c7d411789a7c25e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:49:37.205993Z",
     "start_time": "2024-06-14T10:49:35.639486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "# Label -> Number mapping\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "# Load data\n",
    "for action in actions:\n",
    "    action_dir = os.path.join(DATA_PATH, action)\n",
    "    for sequence in np.array(os.listdir(action_dir)).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            frame_path = os.path.join(action_dir, str(sequence), f\"{frame_num}.npy\")\n",
    "            res = np.load(frame_path)\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# /TODO: Maybe higher test size\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ],
   "id": "23c4cf3e39fd9d6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 75\n",
      "X shape: (75, 30, 1662)\n",
      "y shape: (75, 2)\n",
      "X_train shape: (67, 30, 1662), y_train shape: (67, 2)\n",
      "X_test shape: (8, 30, 1662), y_test shape: (8, 2)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Version 0.1.0\n",
   "id": "6e269a203136e315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# region Version 0.1.0\n",
    "# Define the L2 regularization factor\n",
    "l2_factor = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Input(shape=shape))\n",
    "\n",
    "# Adding LSTM layers with L2 regularization\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Fully connected layer with L2 regularization\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer for num_classes with L2 regularization\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "# Set up CSVLogger\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "\n",
    "# Set up TensorBoard\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]\n",
    "# endregion"
   ],
   "id": "85512f50e9a296d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cda9bdac77566c7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compact LSTM (RNN)",
   "id": "17eebef00fb158ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T11:14:57.175727Z",
     "start_time": "2024-06-05T11:14:57.173786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# region Small LSTM\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax')) \n",
    "# endregion"
   ],
   "id": "99053e38ab77cfe3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Most accuracte",
   "id": "2f8da0e6594a70e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:39:33.973229Z",
     "start_time": "2024-06-14T10:39:33.865212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# region Most accurate Model\n",
    "l2_factor = 0.00\n",
    "dropout = 0.2\n",
    "# These two are for better generalization dropout is 20% and L2 regularization is 0.00\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Masking 0.0 values because keypoints that are not on the screen are 0.0\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "\n",
    "# Statistics\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]\n",
    "# endregion"
   ],
   "id": "9dd7a17a1feeefa",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experimental",
   "id": "3745eada6277bb17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:51:12.740604Z",
     "start_time": "2024-06-14T10:51:12.632129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the L2 regularization factor\n",
    "l2_factor = 0.00\n",
    "dropout = 0.2\n",
    "# These two are for better generalization dropout is 20% and L2 regularization is 0.00\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Masking 0.0 values because keypoints that are not on the screen are 0.0\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "\n",
    "model.add(LSTM(128, kernel_regularizer=l2(l2_factor)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "#model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "\n",
    "# Statistics\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]"
   ],
   "id": "a1a5801cdc7015ce",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:40:38.546036Z",
     "start_time": "2024-06-14T11:40:38.453312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the L2 regularization factor\n",
    "l2_factor = 0.00\n",
    "dropout = 0.2\n",
    "# These two are for better generalization dropout is 20% and L2 regularization is 0.00\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Masking 0.0 values because keypoints that are not on the screen are 0.0\n",
    "model.add(Masking(mask_value=0., input_shape=shape))  # Adjust input shape accordingly\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(LSTM(128))\n",
    "#model.add(Dropout(dropout))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "#model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(l2_factor)))\n",
    "\n",
    "\n",
    "# Statistics\n",
    "csv_logger = CSVLogger('training_log.csv', append=True)\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"sign_language_recognition\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Early stopping callback\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, tensorboard_callback, early_stopping]"
   ],
   "id": "f7a7fe3ee2b5aee0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Execution",
   "id": "b7798f6fcef31617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:40:48.452534Z",
     "start_time": "2024-06-14T11:40:41.092560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=40, validation_data=(X_test, y_test), callbacks=callbacks)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ],
   "id": "b79c1710389baf2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 612ms/step - accuracy: 0.4434 - loss: 0.6923 - val_accuracy: 0.7500 - val_loss: 0.5834\n",
      "Epoch 2/20\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 270ms/step - accuracy: 0.5797 - loss: 0.6883 - val_accuracy: 0.7500 - val_loss: 0.6138\n",
      "Epoch 3/20\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 260ms/step - accuracy: 0.5714 - loss: 0.6850 - val_accuracy: 0.7500 - val_loss: 0.6013\n",
      "Epoch 4/20\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 271ms/step - accuracy: 0.5547 - loss: 0.6853 - val_accuracy: 0.7500 - val_loss: 0.5932\n",
      "Epoch 5/20\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267ms/step - accuracy: 0.5797 - loss: 0.6625 - val_accuracy: 0.7500 - val_loss: 0.6473\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step - accuracy: 0.7500 - loss: 0.5834\n",
      "Test loss: 0.5833909511566162\n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:55:46.428425Z",
     "start_time": "2024-06-14T13:55:46.349099Z"
    }
   },
   "cell_type": "code",
   "source": "id = uuid.uuid4()",
   "id": "3fb2cd7c0af3b735",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:40:20.621Z",
     "start_time": "2024-06-14T10:40:20.565202Z"
    }
   },
   "cell_type": "code",
   "source": "tf.keras.models.save_model(model, f\"ai_{id}.keras\")",
   "id": "7d1fdf6122d61146",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:57:23.863681Z",
     "start_time": "2024-06-14T13:57:20.991128Z"
    }
   },
   "cell_type": "code",
   "source": "tf.saved_model.save(model, 'Conversion/SavedModels/model')",
   "id": "6b9c59ffe5b15c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Conversion/SavedModels/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Conversion/SavedModels/model/assets\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing with OpenCV",
   "id": "2070e80b9fcf7e72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:41:22.479956Z",
     "start_time": "2024-06-14T10:40:32.507690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "model = tf.keras.models.load_model(f\"ai_{id}.keras\")\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Mirror the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    # Ensure that the size of 'res' and 'colors' is the same\n",
    "    if len(res) != len(colors):\n",
    "        print(\"Size of 'res' and 'colors' does not match. Please check your inputs.\")\n",
    "        return output_frame\n",
    "    for num, prob in enumerate(res):\n",
    "        max_prob = np.max(prob)  # get the maximum probability\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(max_prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "colors = [(245,117,16), (117,245,16), (80,22,10), (121,22,76)]\n",
    "cap = cv2.VideoCapture(0)  # Start the webcam\n",
    "ret, image = cap.read()  # Read a frame from the webcam\n",
    "cap.release()  # Close the webcam\n",
    "\n",
    "#plt.figure(figsize=(18,18))\n",
    "#plt.imshow(prob_viz(res, actions, image, colors))\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    collecting = False\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Check if 'g' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('g'):\n",
    "            collecting = True\n",
    "            sequence = []  # Reset the sequence\n",
    "            frame_count = 0  # Reset the frame count\n",
    "\n",
    "        # 2. Prediction logic\n",
    "        if collecting and frame_count < 30:\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(actions[np.argmax(res)])\n",
    "                predictions.append(np.argmax(res))\n",
    "                collecting = False  # Stop collecting after 30 frames\n",
    "\n",
    "                #3. Viz logic\n",
    "                if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                    if res[np.argmax(res)] > threshold:\n",
    "\n",
    "                        if len(sentence) > 0:\n",
    "                            if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\n",
    "                'q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "58f93b36d43473f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718361633.197514   42063 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1718361633.243544   46879 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.3-1pop1~1711635559~22.04~7a9f319), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.57, 6.8.0-76060800daily20240311-generic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1718361634.320688   42063 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1718361634.321748   46902 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.0.3-1pop1~1711635559~22.04~7a9f319), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.57, 6.8.0-76060800daily20240311-generic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 288ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 27ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\n",
      "j\n",
      "Size of 'res' and 'colors' does not match. Please check your inputs.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 78\u001B[0m\n\u001B[1;32m     76\u001B[0m frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mflip(frame, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m# Make detections\u001B[39;00m\n\u001B[0;32m---> 78\u001B[0m image, results \u001B[38;5;241m=\u001B[39m \u001B[43mmediapipe_detection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholistic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# Draw landmarks\u001B[39;00m\n\u001B[1;32m     81\u001B[0m draw_styled_landmarks(image, results)\n",
      "Cell \u001B[0;32mIn[35], line 9\u001B[0m, in \u001B[0;36mmediapipe_detection\u001B[0;34m(image, model)\u001B[0m\n\u001B[1;32m      7\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB) \u001B[38;5;66;03m# COLOR CONVERSION BGR 2 RGB\u001B[39;00m\n\u001B[1;32m      8\u001B[0m image\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m                  \u001B[38;5;66;03m# Image is no longer writeable\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m                 \u001B[38;5;66;03m# Make prediction\u001B[39;00m\n\u001B[1;32m     10\u001B[0m image\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m                   \u001B[38;5;66;03m# Image is now writeable \u001B[39;00m\n\u001B[1;32m     11\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_RGB2BGR) \u001B[38;5;66;03m# COLOR COVERSION RGB 2 BGR\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/AiSL/AiSignLanguage/venv/lib/python3.10/site-packages/mediapipe/python/solutions/holistic.py:160\u001B[0m, in \u001B[0;36mHolistic.process\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[1;32m    137\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001B[39;00m\n\u001B[1;32m    138\u001B[0m \n\u001B[1;32m    139\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;124;03m         \"enable_segmentation\" is set to true.\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks\u001B[38;5;241m.\u001B[39mlandmark:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/AiSL/AiSignLanguage/venv/lib/python3.10/site-packages/mediapipe/python/solution_base.py:340\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[0;34m(self, input_data)\u001B[0m\n\u001B[1;32m    334\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[1;32m    336\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[1;32m    337\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[1;32m    338\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[0;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a010079b526667b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
